{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random', 'log10']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse,random\n",
    "from math import log10\n",
    "%pylab\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data import get_training_set, get_test_set\n",
    "from torch.nn.modules.module import _addindent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1을 사용할시에\n",
    "from net.model import Net\n",
    "# model 2을 사용할시에\n",
    "#from net.model_dw import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda import\n",
    "cuda = True\n",
    "if cuda and not torch.cuda.is_available():\n",
    "    raise Exception(\"No GPU found, please run without --cuda\")\n",
    "torch.manual_seed(random.randint(1,1000))\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(random.randint(1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset import\n",
    "train_set = get_training_set(2,\"BSDS300\")\n",
    "test_set = get_test_set(2,\"BSDS300\")\n",
    "training_data_loader = DataLoader(dataset=train_set, num_workers=6, batch_size=16, shuffle=True)\n",
    "testing_data_loader = DataLoader(dataset=test_set, num_workers=6, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpla=64\n",
    "beta=64\n",
    "gamma=32\n",
    "pruning=0\n",
    "upscale_factor=2\n",
    "train=Net(upscale_factor)\n",
    "weight_name='weight1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weigh\n",
    "model=torch.load(weight_name)\n",
    "keys=model.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "___\n",
    "\n",
    "```\n",
    "self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "self.conv2=nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "self.conv3=nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "self.conv4 = nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "```\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "__upscale_factor는 2 이다__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning method\n",
    "___\n",
    "\n",
    "1. Pruning을 할 수 있는 spot을 선택 \n",
    "1. 1개를 선택하여 prunging 한 후에 Net의 아키텍쳐를 바꾼다.\n",
    "1. L1 norm 을 기준으로 pruning 한다.\n",
    "1. PSNR 값을 구한다.\n",
    "1. retraining을 진행.\n",
    "1. 반복(일정 PSNR 이하 로 내려가기전까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "def modify(net,alpla=64,beta=64,gamma=32):\n",
    "    #global alpla\n",
    "    #global gamma\n",
    "    #global beta\n",
    "    net.conv1 = nn.Conv2d(1, alpla, (5, 5), (1, 1), (2, 2))\n",
    "    net.conv2=nn.Conv2d(alpla, beta, (3, 3), (1, 1), (1, 1))\n",
    "    net.conv3=nn.Conv2d(beta, gamma, (3, 3), (1, 1), (1, 1))\n",
    "    net.conv4 = nn.Conv2d(gamma, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "    \n",
    "def PSNR(net):\n",
    "    avg_psnr = 0\n",
    "    for batch in testing_data_loader:\n",
    "        input, target = Variable(batch[0]), Variable(batch[1])\n",
    "        if cuda:\n",
    "            input = input.cuda()\n",
    "            target=target.cuda()\n",
    "        prediction = net(input)\n",
    "        mse = criterion(prediction, target)\n",
    "        psnr = 10 * log10(1 / mse.data[0])\n",
    "        avg_psnr += psnr\n",
    "    return avg_psnr/len(testing_data_loader)\n",
    "\n",
    "def _train(net,epoch=50):\n",
    "    #net=nn.DataParallel(net) 이렇게 하는게 더 느리다 % time으로 확인\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "    epoch_loss = 0\n",
    "    for iteration, batch in enumerate(training_data_loader, 1):\n",
    "        input, target = Variable(batch[0]), Variable(batch[1])\n",
    "        if cuda:\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(net(input), target)\n",
    "        epoch_loss += loss.data[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.data[0]))\n",
    "    if epoch%10 is 0:\n",
    "        print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_pruning(net,alpla=64,beta=64,gamma=32,retrain=True):\n",
    "    global weight_name\n",
    "    # train을 넣으면 뎀\n",
    "    # 네트워크의 파라미터 값과 pruning 하고자하는 네트워크의 instance를 넣는다\n",
    "    # 그러면 weight를 load하여 모든 weight를 pruning 하여 psnr 값과 weight의 abs sum을 구해서 반환한다.\n",
    "    # pruning 후 retraing 을 한다.\n",
    "    # 만약 weight를 1개 이상 pruning 시에 'weight_name'을 반드시 신경써야한다.\n",
    "    print(\"===> Starting Calculate Pruning\")\n",
    "    _model=torch.load(weight_name)\n",
    "    keys_list=list(_model.keys())\n",
    "    #print(keys_list)\n",
    "    # ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'conv4.weight', 'conv4.bias']\n",
    "    psnr_list=[]\n",
    "    for i in range(0,len(keys_list)-2,2):\n",
    "        if i is 0:\n",
    "            alpla-=1\n",
    "            modify(net,alpla,beta,gamma)\n",
    "        elif i is 2:\n",
    "            alpla+=1\n",
    "            beta-=1\n",
    "            modify(net,alpla,beta,gamma)\n",
    "        elif i is 4:\n",
    "            beta+=1\n",
    "            gamma-=1\n",
    "            modify(net,alpla,beta,gamma)\n",
    "            pruning+=1\n",
    "            _model=torch.load(weight_name)\n",
    "        for j in range(len(_model[keys_list[i]])):\n",
    "            _model=torch.load(weight_name)\n",
    "            weight_matrix=_model[keys_list[i]]\n",
    "            bias_matrix=_model[keys_list[i+1]]\n",
    "            temp_weight=0\n",
    "            if j is 0:\n",
    "                temp_weight=weight_matrix[0].abs().sum()\n",
    "                _model[keys_list[i]]=weight_matrix[1:len(_model[keys_list[i]])]\n",
    "                _model[keys_list[i+1]]=bias_matrix[1:len(_model[keys_list[i]])+1]        \n",
    "            elif j is len(_model[keys_list[i]])-1:\n",
    "                temp_weight=weight_matrix[len(model[keys_list[i]])-1].abs().sum()\n",
    "                _model[keys_list[i]]=weight_matrix[0:len(_model[keys_list[i]])-1]                    \n",
    "                _model[keys_list[i+1]]=bias_matrix[0:len(_model[keys_list[i]])]\n",
    "            else:\n",
    "                temp_weight=weight_matrix[j].abs().sum()\n",
    "                _model[keys_list[i]]=torch.cat((weight_matrix[0:j],weight_matrix[j+1:len(_model[keys_list[i]])]))\n",
    "                _model[keys_list[i+1]]=torch.cat((bias_matrix[0:j],bias_matrix[j+1:len(_model[keys_list[i]])+1]))\n",
    "            if i is 0:\n",
    "                _model[keys_list[i+2]].resize_(alpla,beta,3,3)\n",
    "            elif i is 2:\n",
    "                _model[keys_list[i+2]].resize_(beta,gamma,3,3)\n",
    "            elif i is 4:\n",
    "                _model[keys_list[i+2]].resize_(gamma,upscale_factor ** 2,3,3)\n",
    "            net.load_state_dict(_model)\n",
    "            net=net.cuda()\n",
    "            if retrain is True:\n",
    "                for k in range(0,200):\n",
    "                    _train(net,k)\n",
    "            prnr=PSNR(net)\n",
    "            print('conv:',i,' num:',j,' psnr:',prnr,' size:',temp_weight)\n",
    "            psnr_list.append(tuple([i,j,prnr,temp_weight]))\n",
    "    return psnr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pruning(net):\n",
    "    keys_list=list(keys)\n",
    "    #print(keys_list)\n",
    "    # ['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'conv4.weight', 'conv4.bias']\n",
    "    psnr_list=[]\n",
    "    pruning+=1\n",
    "    for i in range(0,len(keys_list)-1,2):\n",
    "        if i is 0:\n",
    "            alpla-=pruning\n",
    "            modify(train)\n",
    "        elif i is 2:\n",
    "            beta-=pruning\n",
    "            modify(train)\n",
    "        elif i is 4:\n",
    "            gamma-=pruning\n",
    "            modify(train)\n",
    "            pruning+=1\n",
    "        for j in range(len(model[keys_list[i]])):\n",
    "            model=torch.load('weight1')\n",
    "            weight_matrix=model[keys_list[i]]\n",
    "            temp_weight=0\n",
    "            bias_matrix=model[keys_list[i+1]]\n",
    "            if j is 0:\n",
    "                \n",
    "                model[keys_list[i]]=weight_matrix[1:len(model[keys_list[i]])]\n",
    "                temp_weight=weight_matrix[0].abs().sum()\n",
    "                model[keys_list[i+1]]=bias_matrix[1:len(model[keys_list[i]])+1]        \n",
    "            elif j is len(model[keys_list[i]])-1:\n",
    "                temp_weight=weight_matrix[len(model[keys_list[i]])-1].abs().sum()\n",
    "                model[keys_list[i]]=weight_matrix[0:len(model[keys_list[i]])-1]                    \n",
    "                model[keys_list[i+1]]=bias_matrix[0:len(model[keys_list[i]])]\n",
    "            else:\n",
    "                temp_weight=weight_matrix[j].abs().sum()\n",
    "                model[keys_list[i]]=torch.cat((weight_matrix[0:j],weight_matrix[j+1:len(model[keys_list[i]])]))\n",
    "                model[keys_list[i+1]]=torch.cat((bias_matrix[0:j],bias_matrix[j+1:len(model[keys_list[i]])+1]))\n",
    "            if i is 0:\n",
    "                model[keys_list[i+2]].resize_(alpla,beta,3,3)\n",
    "            elif i is 2:\n",
    "                model[keys_list[i+2]].resize_(beta,gamma,3,3)\n",
    "            elif i is 4:\n",
    "                model[keys_list[i+2]].resize_(gamma,upscale_factor ** 2,3,3)\n",
    "            train.load_state_dict(model)\n",
    "            train=train.cuda()\n",
    "            for k in range(0,100):\n",
    "                _train(train,k)\n",
    "            prnr=PSNR(train)\n",
    "            print('conv:',i,' num:',j,' psnr:',prnr,' size:',temp_weight)\n",
    "            psnr_list.append(tuple([i,j,prnr,temp_weight]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(net):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain(net):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_distance(net):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py:514: UserWarning: src is not broadcastable to dst, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  own_state[name].copy_(param)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch 0 Complete: Avg. Loss: 0.2138\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.0311\n",
      "===> Epoch 20 Complete: Avg. Loss: 0.0145\n",
      "===> Epoch 30 Complete: Avg. Loss: 0.0218\n",
      "===> Epoch 40 Complete: Avg. Loss: 0.0077\n",
      "===> Epoch 50 Complete: Avg. Loss: 0.0104\n",
      "===> Epoch 60 Complete: Avg. Loss: 0.0062\n",
      "===> Epoch 70 Complete: Avg. Loss: 0.0062\n",
      "===> Epoch 80 Complete: Avg. Loss: 0.0045\n",
      "===> Epoch 90 Complete: Avg. Loss: 0.0054\n",
      "===> Epoch 100 Complete: Avg. Loss: 0.0037\n",
      "===> Epoch 110 Complete: Avg. Loss: 0.0064\n",
      "===> Epoch 120 Complete: Avg. Loss: 0.0071\n",
      "===> Epoch 130 Complete: Avg. Loss: 0.0035\n",
      "===> Epoch 140 Complete: Avg. Loss: 0.0043\n",
      "===> Epoch 150 Complete: Avg. Loss: 0.0045\n",
      "===> Epoch 160 Complete: Avg. Loss: 0.0035\n",
      "===> Epoch 170 Complete: Avg. Loss: 0.0040\n",
      "===> Epoch 180 Complete: Avg. Loss: 0.0063\n",
      "===> Epoch 190 Complete: Avg. Loss: 0.0062\n",
      "conv: 0  num: 0  psnr: 26.19996456487369  size: 2.926562786102295\n",
      "===> Epoch 0 Complete: Avg. Loss: 0.2235\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.0116\n",
      "===> Epoch 20 Complete: Avg. Loss: 0.0136\n",
      "===> Epoch 30 Complete: Avg. Loss: 0.0142\n",
      "===> Epoch 40 Complete: Avg. Loss: 0.0099\n",
      "===> Epoch 50 Complete: Avg. Loss: 0.0057\n",
      "===> Epoch 60 Complete: Avg. Loss: 0.0114\n",
      "===> Epoch 70 Complete: Avg. Loss: 0.0073\n",
      "===> Epoch 80 Complete: Avg. Loss: 0.0036\n",
      "===> Epoch 90 Complete: Avg. Loss: 0.0087\n",
      "===> Epoch 100 Complete: Avg. Loss: 0.0050\n",
      "===> Epoch 110 Complete: Avg. Loss: 0.0051\n",
      "===> Epoch 120 Complete: Avg. Loss: 0.0061\n",
      "===> Epoch 130 Complete: Avg. Loss: 0.0065\n",
      "===> Epoch 140 Complete: Avg. Loss: 0.0073\n",
      "===> Epoch 150 Complete: Avg. Loss: 0.0055\n",
      "===> Epoch 160 Complete: Avg. Loss: 0.0064\n",
      "===> Epoch 170 Complete: Avg. Loss: 0.0049\n",
      "===> Epoch 180 Complete: Avg. Loss: 0.0064\n",
      "===> Epoch 190 Complete: Avg. Loss: 0.0035\n",
      "conv: 0  num: 1  psnr: 25.41734385229335  size: 3.5600385665893555\n",
      "===> Epoch 0 Complete: Avg. Loss: 0.2052\n",
      "===> Epoch 10 Complete: Avg. Loss: 0.0125\n"
     ]
    }
   ],
   "source": [
    "cal_pruning(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
